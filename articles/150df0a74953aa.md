---
title: "Pythonによるデータ分析の定石"
emoji: "♟️"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: 
  - "python"
  - "NumPy"
  - "Pandas"
  - "matplotlib"
  - "機械学習"
published: false
published_at: "2023-00-00 00:00"
publication_name: "activecore"
---

## はじめに

はじめまして。新卒エンジニアのwinnieです🐥

以前業務でデータ分析や機械学習のモデル構築に携わっていたことがあり、Pythonでデータ分析、特に探索的データ分析（EDA）をする際の手順が自分の中である程度固まってきたので、その知見を記事にまとめました。

この記事は株式会社アクティブコアAdvent Calendar n日目の記事になります！

@[card](https://qiita.com/advent-calendar/2023/activecore)

## 結論

以下の操作を一通りすれば、初手のEDAとしてはかなり良いのではないかと思います。

```python
import pandas as pd
import seaborn as sns

# データ概要の確認
df  # データフレームを表示
df.describe(include="all")
df.info()

# データ分布の確認
sns.pairplot(df)


# カテゴリ変数のカラムがある場合
df[column_name].value_counts()
df.groupby(column_name).describe()

df.isna().sum()
check 3 sigma
df.dropna()
df.drop_duplicates()  # df.duplicated()

delete unused columns or rows

schema validation

df[column].plot()
pd.scatter_matrix(df)
df.corr().style.background_gradient(cmap="coolwarm")

master or transaction data

線形分離可能かのプロット
```

## 解説

### データセットの準備

RecSys 2015のコンペで使われたデータセットを利用していきます[^1]。Kaggleのページから `yoochoose-buys.dat` をダウンロードして、Pandasで読み込んでみましょう。

@[card](https://www.kaggle.com/datasets/chadgostopp/recsys-challenge-2015)

ざっくり言うと、これはECサイトの購買データです。「セッションID」「タイムスタンプ」「商品ID」「価格」「数量」の5つのカラムがあり、誰（セッションID）が何（商品ID）を買ったかと言うことが分かります。余談ですが、アクティブコアはマーケティング領域の会社なので、このようなデータをよく扱います。

```python
import pandas as pd

df = pd.read_csv("yoochoose-buys.dat", header=None, index_col=0, names=[
  "session_id",  # the id of the session. In one session there are one or many buying events. Could be represented as an integer number.
  "timestamp",  # the time when the buy occurred. Format of YYYY-MM-DDThh:mm:ss.SSSZ
  "item_id",  # the unique identifier of item that has been bought. Could be represented as an integer number.
  "price",  # the price of the item. Could be represented as an integer number.
  "quantity",  # the quantity in this buying.  Could be represented as an integer number.
])
```

### データ概要の確認

それでは、まずはデータの概要を確認してみましょう。Jupyter Notebookを使っていれば、 `df` を実行するだけで整形されたデータフレームが見れます。大まかにどのような値が入っているか確認することができますね。また、データフレームのサイズ（ `1,150,753` rows x `4` columns）も下に表示されています。

```python
df
#                            timestamp    item_id  price  quantity
# session_id                                                      
# 420374      2014-04-06T18:44:58.314Z  214537888  12462         1
# 420374      2014-04-06T18:44:58.325Z  214537850  10471         1
# 281626      2014-04-06T09:40:13.032Z  214535653   1883         1
# 420368      2014-04-04T06:13:28.848Z  214530572   6073         1
# 420368      2014-04-04T06:13:28.858Z  214835025   2617         1
# ...                              ...        ...    ...       ...
# 11368701    2014-09-26T07:52:51.357Z  214849809    554         2
# 11368691    2014-09-25T09:37:44.206Z  214700002   6806         5
# 11523941    2014-09-25T06:14:47.965Z  214578011  14556         1
# 11423202    2014-09-26T18:49:34.024Z  214849164   1046         1
# 11423202    2014-09-26T18:49:34.026Z  214560500   5549         1
# 
# [1150753 rows x 4 columns]
```

次に、 `df.describe()`で要約統計量を見てみます。 `include="all"` とすることで量的変数とカテゴリ変数の統計量を両方表示してくれるので、おかしな値が入っていないかを確認しましょう。

1. 各カラムのデータ数 `count` は（表揺れはあるものの）すべてデータ長 `1,150,753` と同じ値になっていますね。ここから、データに `null` は含まれないことが分かります。
2. 価格 `price` や数量 `quantity` の平均値 `mean` と標準偏差 `std` を見てみましょう。価格の標準偏差は `4,652` 円で妥当ですが、モデルに入れるとすると値が大きすぎますね。また、数量の平均値は `0.64` と明らかに少ないです。
3. 価格・数量の最小値 `min` や四分位範囲 `25%` などを見てみます。いずれも、下位50%以下の値はすべて `0` です。これは明らかにおかしいですね。数量の平均値が少ないのもこれが原因だと思われます。最大値 `max` は `334,998` 円と `30` 個でまあまあ妥当といったところです。
4. タイムスタンプ `timestamp` は量的変数として、商品ID `item_id` はカテゴリ変数として扱いたいところですが、それぞれ逆になっています。この部分は調整したいですね。

```python
df.describe(include="all")
#                        timestamp       item_id         price      quantity
# count                    1150753  1.150753e+06  1.150753e+06  1.150753e+06
# unique                   1136477           NaN           NaN           NaN
# top     2014-09-12T17:45:04.693Z           NaN           NaN           NaN
# freq                           5           NaN           NaN           NaN
# mean                         NaN  2.204533e+08  1.423527e+03  6.460865e-01
# std                          NaN  4.897305e+07  4.651549e+03  1.144520e+00
# min                          NaN  2.145073e+08  0.000000e+00  0.000000e+00
# 25%                          NaN  2.147167e+08  0.000000e+00  0.000000e+00
# 50%                          NaN  2.148350e+08  0.000000e+00  0.000000e+00
# 75%                          NaN  2.148498e+08  1.046000e+03  1.000000e+00
# max                          NaN  1.178838e+09  3.349980e+05  3.000000e+01
```

商品IDとタイムスタンプの型を変更します。ついでに、インデックスになっているセッションID `session_id` もカテゴリ変数として扱えるよう `str` 型（Pandasの `object` 型）にしておきます。

```python
df.index = df.index.astype("str")
df["item_id"] = df["item_id"].astype("str")
df["timestamp"] = pd.to_datetime(df["timestamp"])
df

#                                   timestamp    item_id         price      quantity
# count                               1150753    1150753  1.150753e+06  1.150753e+06
# unique                                  NaN      19949           NaN           NaN
# top                                     NaN  643078800           NaN           NaN
# freq                                    NaN      15203           NaN           NaN
# mean    2014-07-05 23:39:07.682516992+00:00        NaN  1.423527e+03  6.460865e-01
# min        2014-04-01 03:05:31.743000+00:00        NaN  0.000000e+00  0.000000e+00
# 25%     2014-05-17 19:40:26.753999872+00:00        NaN  0.000000e+00  0.000000e+00
# 50%        2014-07-11 10:45:34.004000+00:00        NaN  0.000000e+00  0.000000e+00
# 75%        2014-08-22 12:08:15.688000+00:00        NaN  1.046000e+03  1.000000e+00
# max        2014-09-30 02:35:12.859000+00:00        NaN  3.349980e+05  3.000000e+01
# std                                     NaN        NaN  4.651549e+03  1.144520e+00
```

データ型や `null` の数の確認は `df.info()` でもできます。問題なく変換できていますね。

```python
df.info()
# <class 'pandas.core.frame.DataFrame'>
# Index: 1150753 entries, 420374 to 11423202
# Data columns (total 4 columns):
#  #   Column     Non-Null Count    Dtype              
# ---  ------     --------------    -----              
#  0   timestamp  1150753 non-null  datetime64[ns, UTC]
#  1   item_id    1150753 non-null  object             
#  2   price      1150753 non-null  int64              
#  3   quantity   1150753 non-null  int64              
# dtypes: datetime64[ns, UTC](1), int64(2), object(1)
# memory usage: 43.9+ MB
```

### データ分布の確認

続いて、データをプロットして分布を確認してみましょう。データ分布をざっくり把握するにはseabornが便利です。 `sns.pairplot()` でデータフレームを散布図行列の形でプロットすることができます。

```python
import matplotlib.pyplot as plt

sns.pairplot(df)
```

`sns.pairplot()` は量的変数しかプロットしないので、価格と数量しかプロットされていません。商品IDをプロットしても仕方ないのでありがたいですね。グラフを見ると価格と数量ともにガンマ分布のようになっていますが、このままでは扱いづらいので、正規分布のようにしたいところです。

![散布図行列](/images/150df0a74953aa-01.png)

これも余談ですが、ガンマ分布はマーケティングのデータ分析ではよく見る分布です。森岡毅さんの『確率思考の戦略論』[^2]を読むとこの辺りの話が出てきておもしろいと思います。ちなみに作中では負の二項分布として紹介されていますが、連続分布か離散分布かの違いと認識しておくくらいで問題ありません。ちなみに[自分がまとめたスライド](https://speakerdeck.com/yjn279/que-lu-si-kao-nozhan-lue-lun-4b8333cc-8751-448a-87d4-f686a5782ab7)[^3]もあったりします。

## おわりに

[^1]: [RecSys Challenge 2015](https://www.kaggle.com/datasets/chadgostopp/recsys-challenge-2015)
[^2]: [「確率思考の戦略論 ＵＳＪでも実証された数学マーケティングの力」森岡毅 [ノンフィクション] - KADOKAWA](https://www.kadokawa.co.jp/product/321512000337/)
[^3]: [『確率思考の戦略論』 - Speaker Deck](https://speakerdeck.com/yjn279/que-lu-si-kao-nozhan-lue-lun-4b8333cc-8751-448a-87d4-f686a5782ab7)
